{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import re\n",
    "import logging\n",
    "import pickle\n",
    "import torch\n",
    "import torchtext\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext.utils import unicode_csv_reader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../data/News_Category_Dataset_v2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name) as f:\n",
    "    contents = f.read()\n",
    "    contents = '['+contents.replace('}','},')[:-2]+']' \n",
    "    data = json.loads(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'CRIME',\n",
       " 'headline': 'There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " 'authors': 'Melissa Jeltsen',\n",
       " 'link': 'https://www.huffingtonpost.com/entry/texas-amanda-painter-mass-shooting_us_5b081ab4e4b0802d69caad89',\n",
       " 'short_description': 'She left her husband. He killed their children. Just another day in America.',\n",
       " 'date': '2018-05-26'}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32739"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politics = [news for news in data if news['category'] == 'POLITICS']\n",
    "len(politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168114"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_politics = [news for news in data if news['category'] != 'POLITICS']\n",
    "len(non_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(non_politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_politics_balanced = non_politics[0:int(len(politics)/3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(non_politics_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ARTS               105\n",
       "ARTS & CULTURE      87\n",
       "BLACK VOICES       312\n",
       "BUSINESS           382\n",
       "COLLEGE             73\n",
       "COMEDY             353\n",
       "CRIME              204\n",
       "CULTURE & ARTS      58\n",
       "DIVORCE            211\n",
       "EDUCATION           60\n",
       "ENTERTAINMENT     1019\n",
       "ENVIRONMENT         87\n",
       "FIFTY               88\n",
       "FOOD & DRINK       365\n",
       "GOOD NEWS           83\n",
       "GREEN              175\n",
       "HEALTHY LIVING     458\n",
       "HOME & LIVING      256\n",
       "IMPACT             257\n",
       "LATINO VOICES       53\n",
       "MEDIA              172\n",
       "MONEY               99\n",
       "PARENTING          540\n",
       "PARENTS            263\n",
       "QUEER VOICES       394\n",
       "RELIGION           175\n",
       "SCIENCE            143\n",
       "SPORTS             317\n",
       "STYLE              149\n",
       "STYLE & BEAUTY     657\n",
       "TASTE              146\n",
       "TECH               127\n",
       "THE WORLDPOST      245\n",
       "TRAVEL             651\n",
       "WEDDINGS           251\n",
       "WEIRD NEWS         163\n",
       "WELLNESS          1206\n",
       "WOMEN              212\n",
       "WORLD NEWS         147\n",
       "WORLDPOST          170\n",
       "Name: headline, dtype: int64"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category').count()['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced_json = non_politics_balanced + politics\n",
    "random.shuffle(dataset_balanced_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cat(cat):\n",
    "    return '2' if cat == 'POLITICS' else '1'\n",
    "\n",
    "dataset_balanced_all = [[map_cat(news['category']),news['headline']] for news in dataset_balanced_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset_balanced)\n",
    "\n",
    "dataset_balanced = dataset_balanced_all[:LIMIT]\n",
    "train_size = int(0.8*len(dataset_balanced))\n",
    "train_data = dataset_balanced[:train_size]\n",
    "test_data = dataset_balanced[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../data/political-news/train.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(train_data)\n",
    "    \n",
    "with open(\"../data/political-news/test.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerows(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/political-news/'\n",
    "NGRAMS = 2\n",
    "EMBED_DIM = 32\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "LABELS = {\n",
    "     1 : \"non-political\",\n",
    "     2 : \"political\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a helper class to load our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset():\n",
    "    def __init__(self, vocab, data, labels):\n",
    "        super(TextClassificationDataset, self).__init__()\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._vocab = vocab\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self._data:\n",
    "            yield x\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_datasets(root='./data', ngrams=1, vocab=None, include_unk=False):\n",
    "    train_csv_path = root + '/train.csv'\n",
    "    test_csv_path = root + '/test.csv'\n",
    "\n",
    "    if vocab is None:\n",
    "        logging.info('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "            \n",
    "    logging.info('Vocab has {} entries'.format(len(vocab)))\n",
    "    logging.info('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    logging.info('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    \n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Classification\n",
    "==================================\n",
    "\n",
    "A bag of ngrams feature is applied to capture some partial information\n",
    "about the local word order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000lines [00:00, 40773.65lines/s]\n",
      "8000lines [00:00, 14475.60lines/s]\n",
      "2000lines [00:00, 22693.56lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = setup_datasets(root=DATA_DIR, ngrams=NGRAMS)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "The model is composed of the\n",
    "[EmbeddingBag](<https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>)\n",
    "layer and the linear layer. ``nn.EmbeddingBag``\n",
    "computes the mean value of a “bag” of embeddings. The text entries here\n",
    "have different lengths. ``nn.EmbeddingBag`` requires no padding here\n",
    "since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
    "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
    "performance and memory efficiency to process a sequence of tensors.\n",
    "\n",
    "![](../_static/img/text_sentiment_ngrams_model.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLabelTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance of our model\n",
    "--------------------\n",
    "\n",
    "Our model has three labels and therefore the number of classes is three.\n",
    "\n",
    "```\n",
    "{\n",
    "     0 : \"non-political\",\n",
    "     1 : \"political\"\n",
    "}\n",
    "```\n",
    "The vocab size is equal to the length of vocab (including single word\n",
    "and ngrams). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = MultiLabelTextClassifier(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size':VOCAB_SIZE,\n",
    "    'labels': LABELS,\n",
    "    'ngrams': NGRAMS,\n",
    "    'embeddings_dim': EMBED_DIM    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 65124, 'labels': {1: 'non-political', 2: 'political'}, 'ngrams': 2, 'embeddings_dim': 32}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelTextClassifier(\n",
      "  (embedding): EmbeddingBag(65124, 32, mode=mean)\n",
      "  (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(DATA_DIR + 'political_classifier.cfg','wb') as f:\n",
    "    pickle.dump(config,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR + 'political_classifier.vocab','wb') as f:\n",
    "    pickle.dump(train_dataset.get_vocab(),f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batch\n",
    "--------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text entries have different lengths, a custom function\n",
    "generate_batch() is used to generate data batches and offsets. The\n",
    "function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n",
    "The input to ``collate_fn`` is a list of tensors with the size of\n",
    "batch_size, and the ``collate_fn`` function packs them into a\n",
    "mini-batch. \n",
    "\n",
    "The text entries in the original data batch input are packed into a list\n",
    "and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n",
    "The offsets is a tensor of delimiters to represent the beginning index\n",
    "of the individual sequence in the text tensor. Label is a tensor saving\n",
    "the labels of individual text entries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to train the model and evaluate results.\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_function(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test_function(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and run the model\n",
    "-----------------------------------\n",
    "\n",
    "We split the training\n",
    "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). \n",
    "\n",
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)\n",
    "criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n",
    "\n",
    "[SGD](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)\n",
    "implements stochastic gradient descent method as optimizer. The initial\n",
    "learning rate is set to 4.0.\n",
    "\n",
    "[StepLR](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)\n",
    "is used here to adjust the learning rate through epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0323(train)\t|\tAcc: 74.9%(train)\n",
      "\tLoss: 0.0015(valid)\t|\tAcc: 82.2%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0157(train)\t|\tAcc: 89.7%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tAcc: 83.2%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0055(train)\t|\tAcc: 97.6%(train)\n",
      "\tLoss: 0.0013(valid)\t|\tAcc: 82.2%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0017(train)\t|\tAcc: 99.7%(train)\n",
      "\tLoss: 0.0016(valid)\t|\tAcc: 82.5%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0007(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0015(valid)\t|\tAcc: 83.8%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = \\\n",
    "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_function(sub_train_)\n",
    "    valid_loss, valid_acc = test_function(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with test dataset\n",
    "------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "Acc: 85.9%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test_function(test_dataset)\n",
    "print(f'Acc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_DIR + 'political_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo DiCaprio Brings His Mom Irmelin As Date To The Oscars False\n",
      "Trump's Crackdown On Immigrant Parents Puts More Kids In An Already Strained System True\n",
      "---\n",
      "You Are What You Eat True\n",
      "'Trump's Son Should Be Concerned': FBI Obtained Wiretaps Of Putin Ally Who Met With Trump Jr. True\n",
      "---\n",
      "10 Things A Photographer Should Never Do While Photographing A Wedding True\n",
      "Edward Snowden: There's No One Trump Loves More Than Vladimir Putin True\n",
      "---\n",
      "Elderly Woman Fires At Police Robot In Hours-Long Stand-Off: Cops False\n",
      "Booyah: Obama Photographer Hilariously Trolls Trump's 'Spy' Claim True\n",
      "---\n",
      "The Movie Poster For 'Inauguration Day' Looks Pretty Damn Scary True\n",
      "Ireland Votes To Repeal Abortion Amendment In Landslide Referendum True\n",
      "---\n",
      "The Power Of Video To Create Impact True\n",
      "Ryan Zinke Looks To Reel Back Some Critics With 'Grand Pivot' To Conservation True\n",
      "---\n",
      "Kevin Bacon Footloose: Star Does Not Want To Hear His Famous Song At Weddings (VIDEO) True\n",
      "Trump's Scottish Golf Resort Pays Women Significantly Less Than Men: Report True\n",
      "---\n",
      "New York Homeless Speak Out On Government Aid, Shelter Programs True\n",
      "Jack Johnson Was Pardoned, But Taboo Sex Is Still Being Criminalized True\n",
      "---\n",
      "Prison Riot In Mexico Leaves 52 Dead False\n",
      "Bishop Michael Curry Joins Christian March To White House To 'Reclaim Jesus' True\n",
      "---\n",
      "Kate Hudson's Style Evolution: From Boho Wife to Sophisticated Baby Mama (PHOTOS) True\n",
      "How The Chinese Exclusion Act Can Help Us Understand Immigration Politics Today True\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"non-political\",\n",
    "                 2 : \"political\",\n",
    "                }\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "    \n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "for i in range(10):\n",
    "    pred_pol = predict(politics[i]['headline'], model, vocab, 2)\n",
    "    pred_non_pol = predict(non_politics[i]['headline'], model, vocab, 2)\n",
    "    print(non_politics[i]['headline'], pred_non_pol == 1)\n",
    "    print(politics[i]['headline'], pred_pol == 2)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
